{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Author 1:      Jakob Marktl\n",
    "# MatNr 1:       12335939\n",
    "# Author 2:      Christoph Nagy\n",
    "# MatNr 2:       12331569\n",
    "# Author 3:      Maria Mikic\n",
    "# MatNr 3:       12234490\n",
    "# File:          notebook.ipynb\n",
    "# Description:   A simple baseline classifier that makes predictions based on a specified strategy.\n",
    "# Comments:    ... comments for the tutors ...\n",
    "#              ... can be multiline ...\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69934831",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r \"./assignment2/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b6555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1911bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from assignment2.datasetClassifier import (\n",
    "    DatasetHandler,\n",
    "    DecisionTreeClassifier,\n",
    "    GaussianNBClassifier,\n",
    "    KNNClassifier,\n",
    "    LogisticRegressionClassifier,\n",
    "    RandomForestClassifierModel,\n",
    "    SVMClassifier,\n",
    ")\n",
    "from assignment2.datasetPreProcessor import DatasetPreprocessor\n",
    "from assignment2.graphing import Graphing\n",
    "from assignment2.simpleBaselineClassifier import SimpleBaselineClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f600b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"cleaned_dataset.csv\"\n",
    "\n",
    "preprocessor = DatasetPreprocessor(\"student+performance.zip\")\n",
    "preprocessor.to_csv(dataset_path)\n",
    "\n",
    "df: DataFrame = preprocessor.data\n",
    "\n",
    "df.describe()\n",
    "df.value_counts()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03112da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifiers = {\n",
    "    \"GaussianNB\": GaussianNBClassifier(),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNNClassifier(k=5),\n",
    "    \"RandomForest\": RandomForestClassifierModel(n_estimators=100, random_state=0),\n",
    "    \"SVMClassifier\": SVMClassifier(kernel=\"rbf\", c=1.0),\n",
    "    \"LogisticRegression\": LogisticRegressionClassifier(max_iter=5000, random_state=0),\n",
    "    \"SBC_most_frequent\": SimpleBaselineClassifier(\"most_frequent\"),\n",
    "    \"SBC_uniform\": SimpleBaselineClassifier(\"uniform\", random_state=3),\n",
    "    \"SBC_constant\": SimpleBaselineClassifier(\"constant\", constant=3)\n",
    "}\n",
    "\n",
    "dataset_handler = DatasetHandler(dataset_path)\n",
    "graphing = Graphing(dataset_handler)\n",
    "\n",
    "y_preds = {}\n",
    "metrics = {\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(dataset_handler.x_train, dataset_handler.y_train)\n",
    "    y_pred = clf.predict(dataset_handler.x_test)\n",
    "    y_preds[name] = y_pred\n",
    "\n",
    "    metrics[\"Accuracy\"].append(accuracy_score(dataset_handler.y_test, y_pred))\n",
    "    metrics[\"Precision\"].append(precision_score(dataset_handler.y_test, y_pred, average='macro', zero_division=0))\n",
    "    metrics[\"Recall\"].append(recall_score(dataset_handler.y_test, y_pred, average='macro'))\n",
    "    metrics[\"F1 Score\"].append(f1_score(dataset_handler.y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = np.sort(classifiers['RandomForest'].feature_importances)\n",
    "graphing.print_feature_importances(importances)\n",
    "graphing.plot_feature_importances(importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc33ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphing.plot_feature_correspondence([\"G1\", \"G2\", \"absences\", \"Walc\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in classifiers.keys():\n",
    "    y_pred = y_preds[name]\n",
    "    graphing.plot_confusion_matrix(dataset_handler.y_test, y_pred, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a710f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphing.plot_evaluation_metrics(list(classifiers.keys()), metrics, title=\"Evaluation Metrics by Classifier with average=macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_micro = {\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": []\n",
    "}\n",
    "for name in classifiers:\n",
    "    y_pred = y_preds[name]\n",
    "\n",
    "    metrics_micro[\"Accuracy\"].append(accuracy_score(dataset_handler.y_test, y_pred))\n",
    "    metrics_micro[\"Precision\"].append(precision_score(dataset_handler.y_test, y_pred, average=\"micro\"))\n",
    "    metrics_micro[\"Recall\"].append(recall_score(dataset_handler.y_test, y_pred, average=\"micro\"))\n",
    "    metrics_micro[\"F1 Score\"].append(f1_score(dataset_handler.y_test, y_pred, average=\"micro\"))\n",
    "graphing.plot_evaluation_metrics(list(classifiers.keys()), metrics_micro, title=\"Evaluation Metrics by Classifier with average=micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df061725",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_weighted = {\n",
    "    \"Accuracy\": [],\n",
    "    \"Precision\": [],\n",
    "    \"Recall\": [],\n",
    "    \"F1 Score\": []\n",
    "}\n",
    "for name in classifiers:\n",
    "    y_pred = y_preds[name]\n",
    "\n",
    "    metrics_weighted[\"Accuracy\"].append(accuracy_score(dataset_handler.y_test, y_pred))\n",
    "    metrics_weighted[\"Precision\"].append(precision_score(dataset_handler.y_test, y_pred, average=\"weighted\", zero_division=0))\n",
    "    metrics_weighted[\"Recall\"].append(recall_score(dataset_handler.y_test, y_pred, average=\"weighted\"))\n",
    "    metrics_weighted[\"F1 Score\"].append(f1_score(dataset_handler.y_test, y_pred, average=\"weighted\"))\n",
    "graphing.plot_evaluation_metrics(list(classifiers.keys()), metrics_weighted, title=\"Evaluation Metrics by Classifier with average=weighted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
